{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Building a Small Language Model from Scratch\n",
    "\n",
    "[](OPEN IN COLAB)\n",
    "\n",
    "In this notebook, we will build a small **Large Language Model (LLM)** from scratch using PyTorch. Despite the name \"Large\" Language Model, we'll train a small version that demonstrates all the core concepts, so that it fits on Google Colab GPU.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How language models work (predicting the next token)\n",
    "2. Character-level vs word-level tokenization\n",
    "3. Training a generative model\n",
    "4. Generating text and building a chat interface\n",
    "5. Core concepts of Dataset (training/validation/test), Model, Loss, Training/Optimization, Evaluation, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup)\n",
    "2. [Dataset](#2-dataset)\n",
    "3. [Model](#3-model)\n",
    "4. [Loss Function](#4-loss-function)\n",
    "5. [Training](#5-training)\n",
    "6. [Evaluation](#6-evaluation)\n",
    "7. [Chat Interface](#7-chat-interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup \n",
    "<a id=\"1-setup\"></a>\n",
    "\n",
    "First, let's import the necessary libraries and set up our computing device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: NVIDIA GeForce RTX 3090\n",
      "PyTorch version: 2.9.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (training will be slower)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 2. Dataset \n",
    "<a id=\"2-dataset\"></a>\n",
    "\n",
    "### 2.1 What is a Language Model?\n",
    "\n",
    "A language model learns to predict the next token given a sequence of previous tokens:\n",
    "\n",
    "$$P(x_{t+1} | x_1, x_2, ..., x_t)$$\n",
    "\n",
    "For example, given \"The cat sat on the\", a good language model should predict \"mat\" or \"floor\" with high probability.\n",
    "\n",
    "### 2.2 Tokenization\n",
    "\n",
    "**Tokenization** is the process of converting text into numerical tokens. There are several approaches:\n",
    "\n",
    "| Method | Example | Vocabulary Size | Pros | Cons |\n",
    "|--------|---------|-----------------|------|------|\n",
    "| Character-level | \"hello\" → [h, e, l, l, o] | ~100 | Small vocab, handles any word | Long sequences |\n",
    "| Word-level | \"hello world\" → [hello, world] | ~50,000+ | Semantic meaning | Large vocab, OOV issues |\n",
    "| Subword (BPE) | \"unhappy\" → [un, happy] | ~30,000 | Balance of both | More complex |\n",
    "\n",
    "We'll use **character-level tokenization** for simplicity. This means:\n",
    "- Each unique character gets a unique integer ID\n",
    "- The vocabulary is small (around 65 characters for English text)\n",
    "- The model learns to predict one character at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "download-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare dataset...\n",
      "Download complete!\n",
      "Dataset size: 1,115,394 characters\n",
      "\n",
      "First 500 characters:\n",
      "--------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Download a small Shakespeare dataset (about 1MB of text)\n",
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "DATA_FILE = \"shakespeare.txt\"\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(\"Downloading Shakespeare dataset...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Load the text\n",
    "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{'-'*50}\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "### 2.3 Building a Character Tokenizer\n",
    "\n",
    "Our tokenizer needs two functions:\n",
    "1. `encode(text)` → converts text string to list of integers\n",
    "2. `decode(tokens)` → converts list of integers back to text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Test: 'Hello, World!'\n",
      "Encoded: [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded: 'Hello, World!'\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"A simple character-level tokenizer.\n",
    "    \n",
    "    This tokenizer maps each unique character to an integer ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        \"\"\"Build vocabulary from text.\"\"\"\n",
    "        # Get all unique characters and sort them\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Create mappings: character <-> integer\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text string to list of token IDs.\"\"\"\n",
    "        return [self.char_to_id[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert list of token IDs back to text string.\"\"\"\n",
    "        return ''.join([self.id_to_char[i] for i in token_ids])\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = CharacterTokenizer(text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Characters: {''.join(tokenizer.chars)}\")\n",
    "\n",
    "# Test encode/decode\n",
    "test_text = \"Hello, World!\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-class-header",
   "metadata": {},
   "source": [
    "### 2.4 Creating a PyTorch Dataset\n",
    "\n",
    "For language modeling, we create training examples as follows:\n",
    "- **Input (x)**: A sequence of `block_size` tokens\n",
    "- **Target (y)**: The same sequence shifted by one position (next token prediction)\n",
    "\n",
    "Example with `block_size=8`:\n",
    "```\n",
    "Text:   \"The quick brown fox\"\n",
    "Input:  [T, h, e,  , q, u, i, c]  (positions 0-7)\n",
    "Target: [h, e,  , q, u, i, c, k]  (positions 1-8)\n",
    "```\n",
    "\n",
    "This way, at each position, the model learns to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1,003,790\n",
      "Validation samples: 111,476\n",
      "\n",
      "Sample input shape: torch.Size([64])\n",
      "Sample target shape: torch.Size([64])\n",
      "\n",
      "Input:  'First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Al'\n",
      "Target: 'irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All'\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for character-level language modeling.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - x: input sequence of length block_size\n",
    "    - y: target sequence (x shifted by 1 position)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text: The text to create dataset from\n",
    "            tokenizer: Tokenizer to encode the text\n",
    "            block_size: Length of each training sequence (context window)\n",
    "        \"\"\"\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Encode entire text into tokens\n",
    "        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Number of possible starting positions\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of block_size + 1 tokens\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        \n",
    "        # Input is first block_size tokens, target is shifted by 1\n",
    "        x = chunk[:-1]  # First block_size tokens\n",
    "        y = chunk[1:]   # Last block_size tokens (shifted by 1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Hyperparameters for the dataset\n",
    "BLOCK_SIZE = 64  # Context length (how many characters the model sees)\n",
    "\n",
    "# Split data: 90% train, 10% validation\n",
    "split_idx = int(0.9 * len(text))\n",
    "train_text = text[:split_idx]\n",
    "val_text = text[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_text, tokenizer, BLOCK_SIZE)\n",
    "val_dataset = TextDataset(val_text, tokenizer, BLOCK_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Look at one sample\n",
    "x, y = train_dataset[0]\n",
    "print(f\"\\nSample input shape: {x.shape}\")\n",
    "print(f\"Sample target shape: {y.shape}\")\n",
    "print(f\"\\nInput:  '{tokenizer.decode(x.tolist())}'\")\n",
    "print(f\"Target: '{tokenizer.decode(y.tolist())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataloaders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check batch dimensions\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"Batch input shape:  {x_batch.shape}  (batch_size, block_size)\")\n",
    "print(f\"Batch target shape: {y_batch.shape}  (batch_size, block_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 3. Model <a id=\"3-model\"></a>\n",
    "\n",
    "Too many details that we will learn later. You can skip this part.\n",
    "\n",
    "We'll build a **Transformer-based language model**. The key components are:\n",
    "\n",
    "1. **Token Embedding**: Maps token IDs to dense vectors (\n",
    "2. **Positional Encoding**: Adds position information\n",
    "3. **Transformer Blocks**: Self-attention + Feed-forward layers\n",
    "4. **Output Layer**: Projects back to vocabulary size\n",
    "\n",
    "### 3.1 Architecture Overview\n",
    "\n",
    "```\n",
    "Input tokens:    [T, h, e,  , c, a, t]\n",
    "       ↓\n",
    "Token Embedding: Convert each token to a vector\n",
    "       ↓\n",
    "+ Positional Encoding: Add position information\n",
    "       ↓\n",
    "Transformer Block × N:\n",
    "    ├── Self-Attention (look at other positions)\n",
    "    └── Feed-Forward (process each position)\n",
    "       ↓\n",
    "Output Layer: Predict probability of next token\n",
    "       ↓\n",
    "Predictions:  [h, e,  , c, a, t, ...]  (next token for each position)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-header",
   "metadata": {},
   "source": [
    "### 3.2 Self-Attention Mechanism\n",
    "\n",
    "Self-attention allows each position to \"look at\" other positions in the sequence.\n",
    "\n",
    "**The key insight**: To predict the next word, we need to understand the *context*. Self-attention learns which parts of the input are most relevant.\n",
    "\n",
    "For language models, we use **causal (masked) attention**: each position can only attend to previous positions (no peeking at the future!).\n",
    "\n",
    "The attention formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ (Query): \"What am I looking for?\"\n",
    "- $K$ (Key): \"What do I contain?\"\n",
    "- $V$ (Value): \"What information do I provide?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention.\n",
    "    \n",
    "    Causal means each position can only attend to previous positions.\n",
    "    Multi-head means we run multiple attention operations in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k)\n",
    "        \n",
    "        # Linear projections for Q, K, V (combined into one for efficiency)\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask: prevents attending to future positions\n",
    "        # Lower triangular matrix of ones\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time (sequence length), Channels (embed_dim)\n",
    "        \n",
    "        # Compute Q, K, V projections\n",
    "        qkv = self.qkv_proj(x)  # (B, T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Each is (B, T, C)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, T, C) -> (B, num_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores: (B, num_heads, T, T)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply causal mask (set future positions to -inf before softmax)\n",
    "        attn = attn.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values: (B, num_heads, T, head_dim)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape back: (B, num_heads, T, head_dim) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.out_dropout(self.out_proj(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer-block-header",
   "metadata": {},
   "source": [
    "### 3.3 Transformer Block\n",
    "\n",
    "A Transformer block combines:\n",
    "1. **Self-Attention**: Allows positions to communicate\n",
    "2. **Feed-Forward Network**: Processes each position independently\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Residual Connections**: Helps gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer block.\n",
    "    \n",
    "    Structure:\n",
    "        x -> LayerNorm -> Self-Attention -> + x (residual)\n",
    "          -> LayerNorm -> Feed-Forward -> + x (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer normalizations (pre-norm architecture)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attention = CausalSelfAttention(embed_dim, num_heads, block_size, dropout)\n",
    "        \n",
    "        # Feed-forward network (MLP)\n",
    "        # Typically expands by 4x, then projects back down\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),  # Modern activation function\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-model-header",
   "metadata": {},
   "source": [
    "### 3.4 The Complete Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallLLM(nn.Module):\n",
    "    \"\"\"A small GPT-style language model.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token embedding\n",
    "        2. Positional embedding  \n",
    "        3. N transformer blocks\n",
    "        4. Final layer norm\n",
    "        5. Output projection to vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of tokens in vocabulary\n",
    "            embed_dim: Dimension of token embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of transformer blocks\n",
    "            block_size: Maximum sequence length (context window)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Token embedding: vocab_size -> embed_dim\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional embedding: learns a vector for each position\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        \n",
    "        # Dropout after embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, block_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Output projection: embed_dim -> vocab_size\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between token embedding and output projection\n",
    "        # This is a common technique that improves performance and reduces parameters\n",
    "        self.output_proj.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token IDs, shape (batch_size, sequence_length)\n",
    "            \n",
    "        Returns:\n",
    "            Logits for next token prediction, shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Get token embeddings\n",
    "        tok_emb = self.token_embedding(x)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Get positional embeddings\n",
    "        positions = torch.arange(T, device=x.device)  # (T,)\n",
    "        pos_emb = self.position_embedding(positions)  # (T, embed_dim)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = self.dropout(tok_emb + pos_emb)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.ln_final(x)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_proj(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token IDs, shape (batch_size, sequence_length)\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "            temperature: Controls randomness (higher = more random)\n",
    "            top_k: If set, only sample from top k tokens\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs, shape (batch_size, sequence_length + max_new_tokens)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self(idx_cond)\n",
    "            \n",
    "            # Focus on last position\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optional: top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EMBED_DIM = 128     # Embedding dimension (small for fast training)\n",
    "NUM_HEADS = 4       # Number of attention heads\n",
    "NUM_LAYERS = 4      # Number of transformer blocks\n",
    "DROPOUT = 0.1       # Dropout probability\n",
    "\n",
    "# Create the model\n",
    "model = SmallLLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {num_params:,} parameters\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Loss Function <a id=\"4-loss-function\"></a>\n",
    "\n",
    "For language modeling, we use **Cross-Entropy Loss**. This measures how well the predicted probability distribution matches the actual next token.\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(y_i | x_1, ..., x_{i-1})$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of tokens\n",
    "- $y_i$ is the actual next token\n",
    "- $P(y_i | ...)$ is the model's predicted probability for the correct token\n",
    "\n",
    "A related metric is **Perplexity**:\n",
    "$$\\text{Perplexity} = e^{\\mathcal{L}}$$\n",
    "\n",
    "Perplexity can be interpreted as \"how many choices the model is confused between\". Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y):\n",
    "    \"\"\"Compute cross-entropy loss for language modeling.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        x: Input token IDs, shape (batch_size, sequence_length)\n",
    "        y: Target token IDs, shape (batch_size, sequence_length)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    \n",
    "    # Reshape for cross-entropy: (B*T, vocab_size) and (B*T,)\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.view(B * T, V)\n",
    "    targets = y.view(B * T)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test the loss function\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "# Before training, loss should be around -ln(1/vocab_size) = ln(vocab_size)\n",
    "initial_loss = compute_loss(model, x_batch, y_batch)\n",
    "print(f\"Initial loss: {initial_loss.item():.4f}\")\n",
    "print(f\"Expected random loss: {math.log(tokenizer.vocab_size):.4f} (= ln({tokenizer.vocab_size}))\")\n",
    "print(f\"Initial perplexity: {math.exp(initial_loss.item()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training <a id=\"5-training\"></a>\n",
    "\n",
    "Now we'll train the model using standard gradient descent with the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 5\n",
    "EVAL_INTERVAL = 500  # Evaluate every N batches\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional, but helps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS * len(train_loader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, max_batches=50):\n",
    "    \"\"\"Evaluate model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        data_loader: DataLoader for evaluation data\n",
    "        max_batches: Maximum number of batches to evaluate (for speed)\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: Average cross-entropy loss\n",
    "        perplexity: Perplexity = exp(avg_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        loss = compute_loss(model, x, y)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches >= max_batches:\n",
    "            break\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"Train the language model.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = compute_loss(model, x, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (prevents exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Periodic evaluation\n",
    "            if global_step % EVAL_INTERVAL == 0:\n",
    "                val_loss, val_ppl = evaluate(model, val_loader)\n",
    "                train_losses.append(epoch_loss / num_batches)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                print(f\"Step {global_step:5d} | \"\n",
    "                      f\"Train Loss: {epoch_loss/num_batches:.4f} | \"\n",
    "                      f\"Val Loss: {val_loss:.4f} | \"\n",
    "                      f\"Val Perplexity: {val_ppl:.2f}\")\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        val_loss, val_ppl = evaluate(model, val_loader)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Complete\")\n",
    "        print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Perplexity: {val_ppl:.2f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Generate a sample\n",
    "        print(\"Sample generation:\")\n",
    "        prompt = \"ROMEO:\"\n",
    "        generate_text(model, tokenizer, prompt, max_tokens=100)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text given a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    # Generate\n",
    "    output = model.generate(x, max_new_tokens=max_tokens, temperature=temperature, top_k=40)\n",
    "    \n",
    "    # Decode and print\n",
    "    generated_text = tokenizer.decode(output[0].tolist())\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "print(\"Starting training...\\n\")\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([math.exp(l) for l in val_losses], label='Validation', color='orange')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Validation Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation <a id=\"6-evaluation\"></a>\n",
    "\n",
    "Let's evaluate our trained model and explore text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_val_loss, final_ppl = evaluate(model, val_loader, max_batches=100)\n",
    "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"Final Validation Perplexity: {final_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"The king\",\n",
    "    \"Love is\"\n",
    "]\n",
    "\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generate_text(model, tokenizer, prompt, max_tokens=150, temperature=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of temperature\n",
    "print(\"Effect of Temperature on Generation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLower temperature = more deterministic, higher = more random\\n\")\n",
    "\n",
    "prompt = \"HAMLET:\"\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 40)\n",
    "    generate_text(model, tokenizer, prompt, max_tokens=100, temperature=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Chat Interface <a id=\"7-chat-interface\"></a>\n",
    "\n",
    "Now let's create a simple chat interface. Note that our model is trained on Shakespeare, so it will respond in that style rather than as a modern chatbot.\n",
    "\n",
    "To make this work as a \"chat\", we'll format the conversation with speaker labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    \"\"\"A simple chat interface for the language model.\n",
    "    \n",
    "    This formats conversations as a dialogue between USER and ASSISTANT,\n",
    "    then uses the language model to generate responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device, \n",
    "                 max_response_tokens=200, temperature=0.8, \n",
    "                 user_name=\"USER\", assistant_name=\"ASSISTANT\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_response_tokens = max_response_tokens\n",
    "        self.temperature = temperature\n",
    "        self.user_name = user_name\n",
    "        self.assistant_name = assistant_name\n",
    "        \n",
    "        # Conversation history\n",
    "        self.history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Conversation reset.\")\n",
    "    \n",
    "    def format_conversation(self):\n",
    "        \"\"\"Format conversation history as a string.\"\"\"\n",
    "        formatted = \"\"\n",
    "        for role, message in self.history:\n",
    "            formatted += f\"{role}:\\n{message}\\n\\n\"\n",
    "        return formatted\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Send a message and get a response.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user's message\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.history.append((self.user_name, user_input))\n",
    "        \n",
    "        # Create prompt with conversation history\n",
    "        prompt = self.format_conversation()\n",
    "        prompt += f\"{self.assistant_name}:\\n\"\n",
    "        \n",
    "        # Truncate if too long (keep recent context)\n",
    "        max_prompt_len = self.model.block_size - self.max_response_tokens\n",
    "        if len(prompt) > max_prompt_len:\n",
    "            prompt = prompt[-max_prompt_len:]\n",
    "        \n",
    "        # Encode prompt\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        x = torch.tensor([tokens], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        self.model.eval()\n",
    "        output = self.model.generate(\n",
    "            x, \n",
    "            max_new_tokens=self.max_response_tokens,\n",
    "            temperature=self.temperature,\n",
    "            top_k=40\n",
    "        )\n",
    "        \n",
    "        # Decode full output\n",
    "        full_text = self.tokenizer.decode(output[0].tolist())\n",
    "        \n",
    "        # Extract just the response (after the last ASSISTANT:)\n",
    "        response = full_text[len(prompt):]\n",
    "        \n",
    "        # Stop at next speaker label or double newline\n",
    "        for stop_token in [f\"\\n{self.user_name}:\", f\"\\n{self.assistant_name}:\", \"\\n\\n\\n\"]:\n",
    "            if stop_token in response:\n",
    "                response = response[:response.index(stop_token)]\n",
    "        \n",
    "        response = response.strip()\n",
    "        \n",
    "        # Add response to history\n",
    "        self.history.append((self.assistant_name, response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def interactive(self):\n",
    "        \"\"\"Run an interactive chat session.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"Chat with the Shakespeare-trained Language Model\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  'quit' or 'exit' - End the conversation\")\n",
    "        print(\"  'reset' - Clear conversation history\")\n",
    "        print(\"  'history' - Show conversation history\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(f\"{self.user_name}: \").strip()\n",
    "            except EOFError:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.lower() == 'reset':\n",
    "                self.reset()\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'history':\n",
    "                print(\"\\n--- Conversation History ---\")\n",
    "                print(self.format_conversation())\n",
    "                print(\"--- End History ---\\n\")\n",
    "                continue\n",
    "            \n",
    "            response = self.chat(user_input)\n",
    "            print(f\"{self.assistant_name}: {response}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat interface\n",
    "chat = ChatInterface(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    "    max_response_tokens=150,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Demo conversation\n",
    "print(\"Demo Conversation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo_messages = [\n",
    "    \"What is love?\",\n",
    "    \"Tell me about the king.\",\n",
    "    \"How should I live my life?\"\n",
    "]\n",
    "\n",
    "for msg in demo_messages:\n",
    "    print(f\"\\nUSER: {msg}\")\n",
    "    response = chat.chat(msg)\n",
    "    print(f\"ASSISTANT: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run interactive chat (uncomment to use)\n",
    "# Note: This will only work in a Jupyter environment with input support\n",
    "\n",
    "chat.reset()\n",
    "# chat.interactive()  # Uncomment this line to start interactive chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, we built a complete language model from scratch:\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Dataset**: Character-level tokenization converts text to integers. Training examples are (input, target) pairs where target is shifted by one position.\n",
    "\n",
    "2. **Model**: A Transformer-based architecture with:\n",
    "   - Token embeddings (convert token IDs to vectors)\n",
    "   - Positional embeddings (encode position information)\n",
    "   - Causal self-attention (each position attends to previous positions)\n",
    "   - Feed-forward networks (process each position)\n",
    "\n",
    "3. **Loss**: Cross-entropy loss measures how well we predict the next token. Perplexity = exp(loss) gives an interpretable metric.\n",
    "\n",
    "4. **Training**: Standard gradient descent with AdamW optimizer, gradient clipping, and learning rate scheduling.\n",
    "\n",
    "5. **Generation**: Autoregressive sampling - predict one token at a time, append to context, repeat.\n",
    "\n",
    "6. **Chat Interface**: Format conversations with speaker labels and generate responses.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Language models learn to predict the next token given context\n",
    "- The Transformer's self-attention mechanism is the key innovation that enables modern LLMs\n",
    "- Training requires lots of data and compute (our small model is just a demonstration)\n",
    "- Real LLMs (GPT-4, Claude, etc.) use the same principles but are much larger and trained on much more data\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathy's minimal GPT implementation\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual explanation of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Try different values of `EMBED_DIM`, `NUM_HEADS`, and `NUM_LAYERS`. How does model size affect quality and training speed?\n",
    "\n",
    "2. **Different Data**: Download a different text corpus (e.g., Wikipedia, novels, code) and train on it. How does the output style change?\n",
    "\n",
    "3. **Temperature Exploration**: Generate text at different temperatures (0.1 to 2.0). What patterns do you notice?\n",
    "\n",
    "4. **Attention Visualization**: Modify the code to return and visualize attention weights. Which positions does the model attend to?\n",
    "\n",
    "5. **Word-Level Tokenization**: Implement a word-level tokenizer and compare with character-level. What are the trade-offs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
