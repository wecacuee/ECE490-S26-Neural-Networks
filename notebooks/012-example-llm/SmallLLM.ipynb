{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Building a Small Language Model from Scratch\n",
    "\n",
    "[](OPEN IN COLAB)\n",
    "\n",
    "In this notebook, we will build a small **Large Language Model (LLM)** from scratch using PyTorch. Despite the name \"Large\" Language Model, we'll train a small version that demonstrates all the core concepts, so that it fits on Google Colab GPU.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How language models work (predicting the next token)\n",
    "3. Training a generative model\n",
    "4. Generating text and building a chat interface\n",
    "5. Core concepts of Dataset (training/validation/test), Model, Loss, Training/Optimization, Evaluation, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup)\n",
    "2. [Dataset](#2-dataset)\n",
    "3. [Model](#3-model)\n",
    "4. [Loss Function](#4-loss-function)\n",
    "5. [Training](#5-training)\n",
    "6. [Evaluation](#6-evaluation)\n",
    "7. [Chat Interface](#7-chat-interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup \n",
    "<a id=\"1-setup\"></a>\n",
    "\n",
    "First, let's import the necessary libraries and set up our computing device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not load this library: /home/vdhiman/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/lib/libtorchtext.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torch/_ops.py:1488\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: /home/vdhiman/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextT\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/venvs/ece490/lib/python3.10/site-packages/torch/_ops.py:1490\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1490\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load this library: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "\u001b[0;31mOSError\u001b[0m: Could not load this library: /home/vdhiman/.local/venvs/ece490/lib/python3.10/site-packages/torchtext/lib/libtorchtext.so"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.nn as textnn\n",
    "import torchtext.transforms as textT\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (training will be slower)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 2. Dataset \n",
    "<a id=\"2-dataset\"></a>\n",
    "\n",
    "### 2.1 What is a Language Model?\n",
    "\n",
    "A language model learns to predict the next token given a sequence of previous tokens:\n",
    "\n",
    "$$P(x_{t+1} | x_1, x_2, ..., x_t)$$\n",
    "\n",
    "For example, given \"The cat sat on the\", a good language model should predict \"mat\" or \"floor\" with high probability.\n",
    "\n",
    "### 2.2 Tokenization\n",
    "\n",
    "**Tokenization** is the process of converting text into numerical tokens. There are several approaches:\n",
    "\n",
    "| Method | Example | Vocabulary Size | Pros | Cons |\n",
    "|--------|---------|-----------------|------|------|\n",
    "| Character-level | \"hello\" → [h, e, l, l, o] | ~100 | Small vocab, handles any word | Long sequences |\n",
    "| Word-level | \"hello world\" → [hello, world] | ~50,000+ | Semantic meaning | Large vocab, OOV issues |\n",
    "| Subword (BPE) | \"unhappy\" → [un, happy] | ~30,000 | Balance of both | More complex |\n",
    "\n",
    "We'll use **character-level tokenization** for simplicity. This means:\n",
    "- Each unique character gets a unique integer ID\n",
    "- The vocabulary is small (around 65 characters for English text)\n",
    "- The model learns to predict one character at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small Shakespeare dataset (about 1MB of text)\n",
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "DATA_FILE = \"shakespeare.txt\"\n",
    "\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(\"Downloading Shakespeare dataset...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Load the text\n",
    "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{'-'*50}\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "### 2.3 Building a Character Tokenizer\n",
    "\n",
    "Our tokenizer needs two functions:\n",
    "1. `encode(text)` → converts text string to list of integers\n",
    "2. `decode(tokens)` → converts list of integers back to text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"A simple character-level tokenizer.\n",
    "    \n",
    "    This tokenizer maps each unique character to an integer ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        \"\"\"Build vocabulary from text.\"\"\"\n",
    "        # Get all unique characters and sort them\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Create mappings: character <-> integer\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text string to list of token IDs.\"\"\"\n",
    "        return [self.char_to_id[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert list of token IDs back to text string.\"\"\"\n",
    "        return ''.join([self.id_to_char[i] for i in token_ids])\n",
    "\n",
    "# Create our tokenizer\n",
    "tokenizer = CharacterTokenizer(text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Characters: {''.join(tokenizer.chars)}\")\n",
    "\n",
    "# Test encode/decode\n",
    "test_text = \"Hello, World!\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-class-header",
   "metadata": {},
   "source": [
    "### 2.4 Creating a PyTorch Dataset\n",
    "\n",
    "For language modeling, we create training examples as follows:\n",
    "- **Input (x)**: A sequence of `block_size` tokens\n",
    "- **Target (y)**: The same sequence shifted by one position (next token prediction)\n",
    "\n",
    "Example with `block_size=8`:\n",
    "```\n",
    "Text:   \"The quick brown fox\"\n",
    "Input:  [T, h, e,  , q, u, i, c]  (positions 0-7)\n",
    "Target: [h, e,  , q, u, i, c, k]  (positions 1-8)\n",
    "```\n",
    "\n",
    "This way, at each position, the model learns to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for character-level language modeling.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - x: input sequence of length block_size\n",
    "    - y: target sequence (x shifted by 1 position)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text: The text to create dataset from\n",
    "            tokenizer: Tokenizer to encode the text\n",
    "            block_size: Length of each training sequence (context window)\n",
    "        \"\"\"\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Encode entire text into tokens\n",
    "        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Number of possible starting positions\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of block_size + 1 tokens\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        \n",
    "        # Input is first block_size tokens, target is shifted by 1\n",
    "        x = chunk[:-1]  # First block_size tokens\n",
    "        y = chunk[1:]   # Last block_size tokens (shifted by 1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Hyperparameters for the dataset\n",
    "BLOCK_SIZE = 64  # Context length (how many characters the model sees)\n",
    "\n",
    "# Split data: 80% train, 10% validation, 10% test\n",
    "train_val_split_idx = int(0.8 * len(text))\n",
    "val_test_split_idx = int(0.9 * len(text))\n",
    "train_text = text[:train_val_split_idx]\n",
    "val_text = text[train_val_split_idx:val_test_split_idx]\n",
    "test_text = text[val_test_split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_text, tokenizer, BLOCK_SIZE)\n",
    "val_dataset = TextDataset(val_text, tokenizer, BLOCK_SIZE)\n",
    "test_dataset = TextDataset(test_text, tokenizer, BLOCK_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Look at one sample\n",
    "x, y = train_dataset[0]\n",
    "print(f\"\\nSample input shape: {x.shape}\")\n",
    "print(f\"Sample target shape: {y.shape}\")\n",
    "print(f\"\\nInput:  '{tokenizer.decode(x.tolist())}'\")\n",
    "print(f\"Target: '{tokenizer.decode(y.tolist())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataloaders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check batch dimensions\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"Batch input shape:  {x_batch.shape}  (batch_size, block_size)\")\n",
    "print(f\"Batch target shape: {y_batch.shape}  (batch_size, block_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 3. Model \n",
    "<a id=\"3-model\"></a>\n",
    "\n",
    "Too many details that we will learn later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention.\n",
    "    \n",
    "    Causal means each position can only attend to previous positions.\n",
    "    Multi-head means we run multiple attention operations in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k)\n",
    "        \n",
    "        # Linear projections for Q, K, V (combined into one for efficiency)\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask: prevents attending to future positions\n",
    "        # Lower triangular matrix of ones\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time (sequence length), Channels (embed_dim)\n",
    "        \n",
    "        # Compute Q, K, V projections\n",
    "        qkv = self.qkv_proj(x)  # (B, T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Each is (B, T, C)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, T, C) -> (B, num_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores: (B, num_heads, T, T)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply causal mask (set future positions to -inf before softmax)\n",
    "        attn = attn.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values: (B, num_heads, T, head_dim)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape back: (B, num_heads, T, head_dim) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.out_dropout(self.out_proj(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer block.\n",
    "    \n",
    "    Structure:\n",
    "        x -> LayerNorm -> Self-Attention -> + x (residual)\n",
    "          -> LayerNorm -> Feed-Forward -> + x (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer normalizations (pre-norm architecture)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attention = CausalSelfAttention(embed_dim, num_heads, block_size, dropout)\n",
    "        \n",
    "        # Feed-forward network (MLP)\n",
    "        # Typically expands by 4x, then projects back down\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),  # Modern activation function\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-model-header",
   "metadata": {},
   "source": [
    "### 3.4 The Complete Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallLLM(nn.Module):\n",
    "    \"\"\"A small GPT-style language model.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token embedding\n",
    "        2. Positional embedding  \n",
    "        3. N transformer blocks\n",
    "        4. Final layer norm\n",
    "        5. Output projection to vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of tokens in vocabulary\n",
    "            embed_dim: Dimension of token embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of transformer blocks\n",
    "            block_size: Maximum sequence length (context window)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Token embedding: vocab_size -> embed_dim\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional embedding: learns a vector for each position\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        \n",
    "        # Dropout after embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, block_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Output projection: embed_dim -> vocab_size\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between token embedding and output projection\n",
    "        # This is a common technique that improves performance and reduces parameters\n",
    "        self.output_proj.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token IDs, shape (batch_size, sequence_length)\n",
    "            \n",
    "        Returns:\n",
    "            Logits for next token prediction, shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Get token embeddings\n",
    "        tok_emb = self.token_embedding(x)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Get positional embeddings\n",
    "        positions = torch.arange(T, device=x.device)  # (T,)\n",
    "        pos_emb = self.position_embedding(positions)  # (T, embed_dim)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = self.dropout(tok_emb + pos_emb)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.ln_final(x)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_proj(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token IDs, shape (batch_size, sequence_length)\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "            temperature: Controls randomness (higher = more random)\n",
    "            top_k: If set, only sample from top k tokens\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs, shape (batch_size, sequence_length + max_new_tokens)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = self(idx_cond)\n",
    "            \n",
    "            # Focus on last position\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optional: top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EMBED_DIM = 128     # Embedding dimension (small for fast training)\n",
    "NUM_HEADS = 4       # Number of attention heads\n",
    "NUM_LAYERS = 4      # Number of transformer blocks\n",
    "DROPOUT = 0.1       # Dropout probability\n",
    "\n",
    "# Create the model\n",
    "model = SmallLLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {num_params:,} parameters\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-header",
   "metadata": {},
   "source": [
    "## 4. Loss Function \n",
    "<a id=\"4-loss-function\"></a>\n",
    "\n",
    "For language modeling, we use **Cross-Entropy Loss**. This measures how well the predicted probability distribution matches the actual next token.\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(\\hat{y}_i = y_i| x_1, ..., x_{i-1})$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of tokens\n",
    "- $y_i$ is the actual next token\n",
    "- $P(\\hat{y}_i = y_i| ...)$ is the model's predicted probability for the correct token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y):\n",
    "    \"\"\"Compute cross-entropy loss for language modeling.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        x: Input token IDs, shape (batch_size, sequence_length)\n",
    "        y: Target token IDs, shape (batch_size, sequence_length)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    \n",
    "    # Reshape for cross-entropy: (B*T, vocab_size) and (B*T,)\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.view(B * T, V)\n",
    "    targets = y.view(B * T)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Test the loss function\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "# Before training, loss should be around -ln(1/vocab_size) = ln(vocab_size)\n",
    "initial_loss = compute_loss(model, x_batch, y_batch)\n",
    "print(f\"Initial loss: {initial_loss.item():.4f}\")\n",
    "print(f\"Expected random loss: {math.log(tokenizer.vocab_size):.4f} (= ln({tokenizer.vocab_size}))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "<a id=\"5-training\"></a>\n",
    "\n",
    "Now we'll train the model using standard gradient descent with the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 5\n",
    "EVAL_INTERVAL = 500  # Evaluate every N batches\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional, but helps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS * len(train_loader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, max_batches=50):\n",
    "    \"\"\"Evaluate model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        data_loader: DataLoader for evaluation data\n",
    "        max_batches: Maximum number of batches to evaluate (for speed)\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: Average cross-entropy loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        loss = compute_loss(model, x, y)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches >= max_batches:\n",
    "            break\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"Train the language model.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = compute_loss(model, x, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (prevents exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Periodic evaluation\n",
    "            if global_step % EVAL_INTERVAL == 0:\n",
    "                val_loss, val_ppl = evaluate(model, val_loader)\n",
    "                train_losses.append(epoch_loss / num_batches)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                print(f\"Step {global_step:5d} | \"\n",
    "                      f\"Train Loss: {epoch_loss/num_batches:.4f} | \"\n",
    "                      f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        val_loss, val_ppl = evaluate(model, val_loader)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Complete\")\n",
    "        print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Generate a sample\n",
    "        print(\"Sample generation:\")\n",
    "        prompt = \"ROMEO:\"\n",
    "        generate_text(model, tokenizer, prompt, max_tokens=100)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text given a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    # Generate\n",
    "    output = model.generate(x, max_new_tokens=max_tokens, temperature=temperature, top_k=40)\n",
    "    \n",
    "    # Decode and print\n",
    "    generated_text = tokenizer.decode(output[0].tolist())\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "print(\"Starting training...\\n\")\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## 6. Evaluation \n",
    "<a id=\"6-evaluation\"></a>\n",
    "\n",
    "Let's evaluate our trained model and explore text generation. Note that we did not use test dataset in any part during training; not even to decide how good our model was. In general you need to be disciplined about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_test_loss, final_ppl = evaluate(model, test_loader, max_batches=100)\n",
    "print(f\"Final Test Loss: {final_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"The king\",\n",
    "    \"Love is\"\n",
    "]\n",
    "\n",
    "print(\"Text Generation Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generate_text(model, tokenizer, prompt, max_tokens=150, temperature=0.8)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temperature-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of temperature\n",
    "print(\"Effect of Temperature on Generation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLower temperature = more deterministic, higher = more random\\n\")\n",
    "\n",
    "prompt = \"HAMLET:\"\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 40)\n",
    "    generate_text(model, tokenizer, prompt, max_tokens=100, temperature=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-header",
   "metadata": {},
   "source": [
    "## 7. Chat Interface \n",
    "<a id=\"7-chat-interface\"></a>\n",
    "\n",
    "Now let's create a simple chat interface. Note that our model is trained on Shakespeare, so it will respond in that style rather than as a modern chatbot.\n",
    "\n",
    "To make this work as a \"chat\", we'll format the conversation with speaker labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    \"\"\"A simple chat interface for the language model.\n",
    "    \n",
    "    This formats conversations as a dialogue between USER and ASSISTANT,\n",
    "    then uses the language model to generate responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device, \n",
    "                 max_response_tokens=200, temperature=0.8, \n",
    "                 user_name=\"USER\", assistant_name=\"ASSISTANT\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_response_tokens = max_response_tokens\n",
    "        self.temperature = temperature\n",
    "        self.user_name = user_name\n",
    "        self.assistant_name = assistant_name\n",
    "        \n",
    "        # Conversation history\n",
    "        self.history = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Conversation reset.\")\n",
    "    \n",
    "    def format_conversation(self):\n",
    "        \"\"\"Format conversation history as a string.\"\"\"\n",
    "        formatted = \"\"\n",
    "        for role, message in self.history:\n",
    "            formatted += f\"{role}:\\n{message}\\n\\n\"\n",
    "        return formatted\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Send a message and get a response.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The user's message\n",
    "            \n",
    "        Returns:\n",
    "            The model's response\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.history.append((self.user_name, user_input))\n",
    "        \n",
    "        # Create prompt with conversation history\n",
    "        prompt = self.format_conversation()\n",
    "        prompt += f\"{self.assistant_name}:\\n\"\n",
    "        \n",
    "        # Truncate if too long (keep recent context)\n",
    "        max_prompt_len = self.model.block_size - self.max_response_tokens\n",
    "        if len(prompt) > max_prompt_len:\n",
    "            prompt = prompt[-max_prompt_len:]\n",
    "        \n",
    "        # Encode prompt\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        x = torch.tensor([tokens], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        self.model.eval()\n",
    "        output = self.model.generate(\n",
    "            x, \n",
    "            max_new_tokens=self.max_response_tokens,\n",
    "            temperature=self.temperature,\n",
    "            top_k=40\n",
    "        )\n",
    "        \n",
    "        # Decode full output\n",
    "        full_text = self.tokenizer.decode(output[0].tolist())\n",
    "        \n",
    "        # Extract just the response (after the last ASSISTANT:)\n",
    "        response = full_text[len(prompt):]\n",
    "        \n",
    "        # Stop at next speaker label or double newline\n",
    "        for stop_token in [f\"\\n{self.user_name}:\", f\"\\n{self.assistant_name}:\", \"\\n\\n\\n\"]:\n",
    "            if stop_token in response:\n",
    "                response = response[:response.index(stop_token)]\n",
    "        \n",
    "        response = response.strip()\n",
    "        \n",
    "        # Add response to history\n",
    "        self.history.append((self.assistant_name, response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def interactive(self):\n",
    "        \"\"\"Run an interactive chat session.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"Chat with the Shakespeare-trained Language Model\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  'quit' or 'exit' - End the conversation\")\n",
    "        print(\"  'reset' - Clear conversation history\")\n",
    "        print(\"  'history' - Show conversation history\")\n",
    "        print(\"=\"*60)\n",
    "        print()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(f\"{self.user_name}: \").strip()\n",
    "            except EOFError:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.lower() == 'reset':\n",
    "                self.reset()\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'history':\n",
    "                print(\"\\n--- Conversation History ---\")\n",
    "                print(self.format_conversation())\n",
    "                print(\"--- End History ---\\n\")\n",
    "                continue\n",
    "            \n",
    "            response = self.chat(user_input)\n",
    "            print(f\"{self.assistant_name}: {response}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat interface\n",
    "chat = ChatInterface(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    "    max_response_tokens=150,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Demo conversation\n",
    "print(\"Demo Conversation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo_messages = [\n",
    "    \"What is love?\",\n",
    "    \"Tell me about the king.\",\n",
    "    \"How should I live my life?\"\n",
    "]\n",
    "\n",
    "for msg in demo_messages:\n",
    "    print(f\"\\nUSER: {msg}\")\n",
    "    response = chat.chat(msg)\n",
    "    print(f\"ASSISTANT: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run interactive chat (uncomment to use)\n",
    "# Note: This will only work in a Jupyter environment with input support\n",
    "\n",
    "chat.reset()\n",
    "# chat.interactive()  # Uncomment this line to start interactive chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c800501-9918-4d7c-9f7d-e79cd77f4793",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built a complete language model from scratch:\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Dataset**: Character-level tokenization converts text to integers. Training examples are (input, target) pairs where target is shifted by one position.\n",
    "\n",
    "2. **Model**: A Transformer-based architecture\n",
    "\n",
    "    We will learn more  on this later in the course. For the curious\n",
    "\n",
    "4. **Loss**: Cross-entropy loss measures how well we predict the next token. \n",
    "\n",
    "5. **Training**: Standard gradient descent with AdamW optimizer, gradient clipping, and learning rate scheduling.\n",
    "\n",
    "6. **Generation**: Autoregressive sampling - predict one token at a time, append to context, repeat.\n",
    "\n",
    "7. **Chat Interface**: Format conversations with speaker labels and generate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad36dd2-8bda-464f-8d09-246d4a90cf3e",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Language models learn to predict the next token given context\n",
    "- Training requires lots of data and compute (our small model is just a demonstration)\n",
    "- Real LLMs (GPT-4, Claude, etc.) use the same principles but are much larger and trained on much more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathy's minimal GPT implementation\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual explanation of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Different Data**: Download a different text corpus (e.g., Wikipedia, novels, code) and train on it. You can get datasets from the [torchtext module here](https://docs.pytorch.org/text/stable/datasets.html); pick the ones under the \"unsupervised Learning category.\".\n",
    "\n",
    "3. **Different Model**: Try a different model like LSTM on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982d6c9-a68a-40e6-802e-b3ad2792977a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
