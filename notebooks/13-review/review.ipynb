{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dad0d2-8824-4971-bc9e-662471adc12f",
   "metadata": {},
   "source": [
    "# Final exam review: Question bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ec556-1265-406a-952e-9c6b58263f54",
   "metadata": {},
   "source": [
    "## Final exam instructions\n",
    "1. The final exam is on 12/16/2025, Tuesday from 9:30AM - 11:30AM in Shibles Hall 217\n",
    "2. It covers all the syllabus including midterm 1\n",
    "3. Total marks will be 120.\n",
    "4. Total time allowed is 120 min.\n",
    "5. One 8x11 in page cheatsheet is allowed with both sides.\n",
    "6. Non-programmable calculators are allowed but not needed.\n",
    "7. Laptops, computers, mobile phones are not allowed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d0c6d-449b-4c8e-9309-b347db5532bf",
   "metadata": {},
   "source": [
    "What did we learn in this course?\n",
    "\n",
    "## Midterm 1\n",
    "\n",
    "1. Python basics: Please go through common Python idioms\n",
    "   - [Python_1.ipynb](../01-py-intro/Python_1.ipynb)\n",
    "   - [Python_2.ipynb](../01-py-intro/Python_2.ipynb) \n",
    "3. Numpy basics: Please understand how broadcasting works. Understand the differences between `*` and `@` operators.\n",
    "   - [NumpyTutorial.ipynb](../02-linear-models/01-Python_3.ipynb)\n",
    "   - [PytorchTutorial.ipynb](../06-pytorch/NumpyTutorial-Pytorched.ipynb)\n",
    "     \n",
    "5. Linear Regression by vector derivatives:\n",
    "   (Please understand vector derivatives, and minimizing quadratic\n",
    "   functions using vector calculus.)\n",
    "   - [LinearModels.ipynb](../02-linear-models/00-LinearModels.ipynb),\n",
    "   - [PlaneFitProblem](../02-linear-models/02-PlaneFitProblem.ipynb),\n",
    "   - [Practice Problems for Midterm 1](<../04-review/Practice Problems Bank.ipynb>),\n",
    "     \n",
    "8. 1-Layer Neural Network: Understand hinge loss and its derivatives.\n",
    "    [Perceptron3.ipynb](../02-linear-models/40-Perceptron3.ipynb)\n",
    "\n",
    "9. Midterm 2 review: [Practice Problems](<../08-review/Midterm 2 Review.ipynb>)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c63e3-bc79-4b80-b80f-f9e224245de4",
   "metadata": {},
   "source": [
    "### Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff725e-c377-4bac-8f41-f62480035654",
   "metadata": {},
   "source": [
    "##### Question: \n",
    "Given two random variables $X$ and $Y$ specify the relationship between $P(X|Y)$ and $P(Y|X)$ using Bayes rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2685fe4-4a5f-4cda-b4eb-e723af6cd3aa",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf46e4-aaf3-4792-9eb5-0805405e9be8",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Using CIFAR 10 dataset $\\calD$ you want to find your neural network with weights $\\bfW$. Define Prior, Likelihood and Posterior in terms of the weights $\\bfW$ and data $\\calD$. Write Bayes rule to determin posterior from likehood and prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebe447-046d-4e1f-8905-b9ba1c2ddf94",
   "metadata": {},
   "source": [
    "##### Answer: \n",
    "\n",
    "- Prior is $P(\\bfW)$ i.e. the probability of weights $\\bfW$ before we have seen the dataset.\n",
    "- Likelihood is $P(\\calD|\\bfW)$ i.e. the probability of observing the dataset once if we pick a particular choice of weighs $\\bfW$.\n",
    "- Posterior is $P(\\bfW|\\calD)$ i.e the probability of chosing weights given the dataset $\\calD$.\n",
    "\n",
    "By Bayes rule, we have:\n",
    "\n",
    "$$ P(\\bfW|\\calD) = \\frac{P(\\calD|\\bfW)P(\\bfW)}{P(\\calD)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798cb2f-490a-47e9-abed-5a0376071944",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "How can regularization can be interpretted as an application of the Bayes theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c7202-8d4f-4347-bd8b-28fe2dee8902",
   "metadata": {},
   "source": [
    "Given the dataset $\\calD = \\{ (\\bfx_1, y_1), \\dots, (\\bfx_n, y_n) \\}$, a model $\\hat{y}_i = f(\\bfx_i; \\bfW)$, a regularizer $R(\\bfW)$ and a loss function $l(y_i, \\hat{y}_i)$, show that the following optimization problem can be interpreted as maximum-a-posteriori estimation. In the process show that for the interpretation, we need the IID (independently, identically distributed) assumption over the dataset. List any other assumptions that you need for the interpretation.\n",
    "\n",
    "$$ \\bfW^* = \\arg~\\min_\\bfW \\sum_{i=1}^n l(y_i, f(\\bfx_i; \\bfW)) + \\lambda R(\\bfW),$$\n",
    "\n",
    "where $\\lambda$ is some positive constant that balances between the loss function and the regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5257b19-850d-49a0-835d-71299468aa8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Answer:\n",
    "Let the $\\bfx_i$ and $y_i$ be random vectors for all $i$. Model the probability distribution as a negative log of the loss function:\n",
    "\n",
    "$$ P((\\bfx_i, y_i)| \\bfW) = \\frac{1}{Z} \\exp(-l(y_i, f(\\bfx_i; \\bfW)).$$\n",
    "\n",
    "If the samples are IID, then we can write the probability of the entire dataset as products of sample probabilities\n",
    "\n",
    "$$ P(\\calD|\\bfW) = \\prod_{i=1}^n P((\\bfx_i, y_i)| \\bfW) $$\n",
    "\n",
    "$$ P(\\calD|\\bfW) = \\prod_{i=1}^n \\frac{1}{Z} \\exp(-l(y_i, f(\\bfx_i; \\bfW)).$$\n",
    "\n",
    "A product of exponents is the summation of their powers,\n",
    "\n",
    "$$ P(\\calD|\\bfW) = \\frac{1}{Z} \\exp(-\\sum_{i=1}^n l(y_i, f(\\bfx_i; \\bfW)).$$\n",
    "\n",
    "Denote $$ L(\\calD; \\bfW) = \\sum_{i=1}^n l(y_i, f(\\bfx_i; \\bfW).$$\n",
    "\n",
    "The original optimization problem can be written as:\n",
    "$$ \\bfW^* = \\arg~\\min_\\bfW L(\\calD; \\bfW) + \\lambda R(\\bfW)$$\n",
    "\n",
    "Taking negative exponent on both sides turns the problem into a maximization problem because $\\exp(-y)$ is a monotonically decreasing function.\n",
    "$$ \\bfW^* = \\arg~\\max_\\bfW \\exp(-L(\\calD; \\bfW))\\exp(-\\lambda R(\\bfW))$$\n",
    "\n",
    "The first term is the same as maximizing the likelihood $P(\\calD|\\bfW)$. If we interpret the second term as a prior:\n",
    "$$ P(\\bfW) = \\frac{1}{Z'} \\exp(-\\lambda R(\\bfW)),$$\n",
    "\n",
    "then we can rewrite the original optimization problem as\n",
    "\n",
    "$$ \\bfW^* = \\arg~\\max_\\bfW P(\\calD|\\bfW) P(\\bfW) $$\n",
    "\n",
    "By Bayes theorem $P(\\calD|\\bfW) P(\\bfW) = P(\\bfW|\\calD)P(\\calD)$, hence we can write the optimization problem as maximizing the posterior\n",
    "\n",
    "$$ \\bfW^* = \\arg~\\max_\\bfW P(\\bfW|\\calD) P(\\calD).$$\n",
    "\n",
    "We can ignore the evidence term $P(\\calD)$, because it is independent of $\\bfW$ the optimization variable. The original problem reduces to maximizing the posterior, hence maximum a posteriori:\n",
    "\n",
    "$$ \\bfW^* = \\arg~\\max_\\bfW P(\\bfW|\\calD)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd1b4b-dc7d-4a2a-a49b-89362f2ac216",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Question:\n",
    "You are doing 0-1 binary classification on the MNIST hand digit classification task. Let detecting the digit 1 be considered the positive class. Using this example, define the terms False negative, False positive, Accuracy, Precision, recall, F1-score.\n",
    "\n",
    "##### Answer:\n",
    "- False negative: is the case when the predictor classifies the digit as negative i.e. 0 but it is actually 1.\n",
    "- False positive: is the case when the predictor classifies the digit as positive i.e. 1 but it is actually 0.\n",
    "- Accuracy: Total correct classifictions/Total samples .\n",
    "- Precision: True positives / Predicted positives.\n",
    "- Recall: True positives / All actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802b48d-e904-437f-aca1-629ad06cd2c6",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "What is the diference between Empirical risk and Expected risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26171cfd-aaf2-4702-8bb8-c77d93883dad",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "Empirical risk is the average loss over the dataset while Expected risk is the expectation of the loss when the inputs and outputs are treated as random variables. \n",
    "\n",
    "For example, if a dataset $\\calD = \\{(\\bfx_1, y_1), \\dots, (\\bfx_n, y_n)\\}$ with $\\bfx_i \\in \\bbR^n$ and $y_i \\in \\bbR$ is given and the model $f(\\bfx_i; \\bfW)$ predicts labels $\\hat{h}_i = f(\\bfx_i; \\bfW)$. Then the empirical risk is the average loss over the dataset samples,\n",
    "\n",
    "$$ R_\\calD(\\bfW) = \\frac{1}{n} \\sum_{i=1}^n loss(\\hat{y}_i, y_i). $$\n",
    "\n",
    "On the other hand to compute Expected risk, we assume that the dataset $\\calD$ was independently sampled (IID assumption) from a distribution $\\bbP_{X,Y}$ where the input $\\bfx_i$ is an instance of random variable $X$ and $y_i$ is an instance of random variable $Y$.\n",
    "\n",
    "$$ R(\\bbP_{X,Y}, \\bfW) = \\bbE_{X,Y} [ loss(\\hat{Y}, Y) ], $$\n",
    "where $\\hat{Y} = f(X; \\bfW)$.\n",
    "\n",
    "If $X$ and $Y$ are continous random variables, then the expectation is the weighted integraal over all possible values of values of $X$ and $Y$ weighted by the joint probability density function (PDF) of $X$ and $Y$.\n",
    "\n",
    "$$ R(\\bbP_{X,Y}, \\bfW) = \\bbE_{X,Y} [ loss(\\hat{Y}, Y) ] = \\int_{x \\in \\Omega_X} \\int_{y \\in \\Omega_Y} loss(\\hat{y}, y) f_{X,Y}(x, y) dx dy, $$\n",
    "where $\\hat{y} = f(x; \\bfW)$ and $\\Omega_X$ and $\\Omega_Y$ is the sample space for $X$ and $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2429d4d-2a8b-4b3d-bbe5-2600a0412a8d",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptrons (MLPs): \n",
    "\n",
    "[MLP Using Pytorch.ipynb](<../06-pytorch/MLP\\ Using\\ Pytorch.ipynb>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b936c65-5725-4270-b95f-71e5b5a02368",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Question:\n",
    "Count the number of trainable parameters in a two layer MLP with $3*30*30=2700$ size input vector, 10 hidden units and 10 output units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31025ab-9463-4aad-8eb4-a40997b3d56e",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "This MLP can be written as\n",
    "\\begin{align}\n",
    "\\bfy = MLP(\\bfx) = \\bfW_2 a(\\bfW_1 \\bfx + \\bfb_1) + \\bfb_2,\n",
    "\\end{align}\n",
    "with two layers,\n",
    "\\begin{align}\n",
    "\\bfh = a(\\bfW_1 \\bfx + \\bfb_1) \\in \\bbR^{10}\\\\\n",
    "\\bfy = \\bfW_2 \\bfh + \\bfb_2 \\in \\bbR^{10}\n",
    "\\end{align}\n",
    "\n",
    "where the input $\\bfx \\in \\bbR^{2700}$ and output $\\bfy \\in \\bbR^{10}$ and the hidden units $\\bfh \\in \\bbR^{10}$.\n",
    "\n",
    "From the first layer dimensions we get, $\\bfb_1 \\in \\bbR^{10}$ and $\\bfW_1 \\in \\bbR^{10 \\times 2700}$.\n",
    "\n",
    "From the second layer dimensions we get, \n",
    " $\\bfb_2 \\in \\bbR^{10}$ and $\\bfW_2 \\in \\bbR^{10 \\times 10}$.\n",
    "\n",
    " Total number of trainable parameters are $10*2700 + 10 + 10*10 + 10 = 27,120$\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751a36c-962f-4b86-8e0a-eb32f490e66e",
   "metadata": {},
   "source": [
    "## Automatic Differentiation:\n",
    "\n",
    "- [Autograd.ipynb](../03-autograd/Autograd.ipynb)\n",
    "- [AutogradNumpy.ipynb](../03-autograd/AutogradNumpy.ipynb)\n",
    "- [microtorch_mlp.ipynb](../05-mlp/microtorch_mlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c5d1c-8842-4da5-80ee-8c494b68763c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Question:\n",
    "\n",
    "Compare the computational complexity of forward-mode differentiation with reverse-mode differentiation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bfa0e-ee32-428b-a799-8eb61d202b3c",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "In forward diff, we compute computaional complexity from input side to the output side.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\left(\\frac{\\p \\bff}{\\p \\bfg} \\left( \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} \\right)\\right)$$\n",
    "\n",
    "The first two matrix multiplications  $X_{p \\times n} = \\left( \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} \\right)$ are of the size $p \\times m$ and $m \\times n$, resulting in $O(pmn)$ complexity.\n",
    "\n",
    "The second two matrix multiplications $\\left(\\frac{\\p \\bff}{\\p \\bfg} X_{p \\times n} \\right)$ are of the size $q \\times p$ and $p \\times n$, resulting in $O(qpn)$ complexity.\n",
    "\n",
    "The total computational complexity of forward differentiation is $O(qpn + pmn) = O((qp+pm)n)$.\n",
    "\n",
    "For a longer chain of functions of Jacobians of shape $q_i \\times p_i$ with ($p_i = q_{i-1}$).\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff_n(\\dots \\bff_2(\\bff_1(\\bfx))) = \n",
    "\\frac{\\p \\bff_n}{\\p \\bff_{n-1}}_{q_n \\times p_{n}}  \n",
    "\\dots\n",
    "\\frac{\\p \\bff_2}{\\p \\bff_1}_{q_1 \\times p_1}\n",
    "\\frac{\\p \\bff_1}{\\p \\bfx}_{q_0 \\times p_0} $$\n",
    "\n",
    "We get a computational complexity that looks like  $O( (\\sum_{i=1}^n q_i p_i) p_0)$. Note that the size of input $p_0$ is the only common factor for the entire chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2f25e-0644-4798-af98-879d9a87e24b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Computational complexity of reverse-mode diff**\n",
    "In reverse-mode diff, we compute computaional complexity from input side to the output side.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\left(\\left(\\frac{\\p \\bff}{\\p \\bfg}  \\frac{\\p \\bfg}{\\p \\bfh}\\right) \\frac{\\p \\bfh}{\\p \\bfx} \\right)$$\n",
    "\n",
    "The first two matrix multiplications  $X_{q \\times p} = \\left( \\frac{\\p \\bff}{\\p \\bfg}  \\frac{\\p \\bfg}{\\p \\bfh} \\right)$ are of the size $q \\times p$ and $p \\times m$, resulting in $O(qpm)$ complexity.\n",
    "\n",
    "The second two matrix multiplications $\\left(X_{q \\times p}\\frac{\\p \\bfh}{\\p \\bfx}  \\right)$ are of the size $q \\times p$ and $p \\times n$, resulting in $O(qpn)$ complexity.\n",
    "\n",
    "The total computational complexity of forward differentiation is $O(qpm + qmn) = O(q(pm+mn))$. \n",
    "\n",
    "For a longer chain of functions of Jacobians of shape $q_i \\times p_i$ with ($p_i = q_{i-1}$).\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff_n(\\dots \\bff_2(\\bff_1(\\bfx))) = \n",
    "\\frac{\\p \\bff_n}{\\p \\bff_{n-1}}_{q_n \\times p_{n}}  \n",
    "\\dots\n",
    "\\frac{\\p \\bff_2}{\\p \\bff_1}_{q_1 \\times p_1}\n",
    "\\frac{\\p \\bff_1}{\\p \\bfx}_{q_0 \\times p_0} $$\n",
    "\n",
    "We get a computational complexity that looks like  $O( q_n (\\sum_{i=0}^{n-1} q_i p_i))$. Note that the size of output $q_n$ is the only common factor for the entire chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f970fe-6cb6-41fa-89fb-b8c3dd39c66b",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "Describe Vector Jacobian product (vjp) for reverse-mode differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab494a0b-9144-4bd7-bfb6-76075b8865a7",
   "metadata": {},
   "source": [
    "Typical output of a neural network is a loss function. Loss function is always a scalar. Most neural network libraries implement reverse-mode differentiation only for a scalar output.\n",
    "\n",
    "Hence, the first Jacobian on the output side of chain rule is a row-vector.\n",
    "$$ \\frac{\\p }{\\p \\bfx} l(\\bff(\\bfg(\\bfx)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p \\bff}{\\p \\bfg}\\frac{\\p \\bfg}{\\p \\bfx},$$\n",
    "$\\bfg(\\bfx): \\bbR^n \\to \\bbR^p$, $\\bff(\\bfg): \\bbR^p \\to \\bbR^q$ and $l(\\bff): \\bbR^q \\to \\bbR$.\n",
    "\n",
    "When you are writing a programmatic derivative function for reverse mode differentation, the function does two things:\n",
    "\n",
    "1. Compute the local Jacobian of the function for example $\\frac{\\p \\bff}{\\p \\bfg}$.\n",
    "2. Left multiply the Jacobian with a row-vector of accumulated derivative so far. For example, $\\frac{\\p l}{\\p \\bff} \\frac{\\p \\bff}{\\p \\bfg}$.\n",
    "\n",
    "The template of the function is like this:\n",
    "```python\n",
    "def g(arg1, arg2):\n",
    "    # Compute g\n",
    "    return g \n",
    "    \n",
    "def g_vjp(arg1, arg2, dl_dg):\n",
    "    # Compute vector Jacobian product with respect to each oargument\n",
    "    return dl_arg1, dl_arg2\n",
    "```\n",
    "\n",
    "If you are given a function $\\bfg(\\bfx)$, and you want to implement `vjp` function for it. It is often easier to imagine a sclar loss function $l(\\bfg(\\bfx))$ whose accumulated gradient $\\frac{\\p l}{\\p \\bfg}$ is given as an input argument. The function `vjp` returns the derivative of the loss function with respect to the inputs,\n",
    "$$\\frac{\\p}{\\p \\bfx} l(\\bfg(\\bfx)) = \\frac{\\p l}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfx},$$\n",
    "which looks like a vector Jacobian product, but you are free to not compute the Jacobian separately. Sometimes it is computationally harder to compute the jacobian separately then multiply it by the vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c6b16-e5d5-42d1-a235-ba5705cb4c7f",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Write the  vector jacobian products for vector addition\n",
    "$$\\bff(\\bfa, \\bfb) = \\bfa + \\bfb$$ where $\\bfa, \\bfb, \\bff \\in \\bbR^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23176829-44a1-48e4-8fa9-c822866a8ba7",
   "metadata": {},
   "source": [
    "\n",
    "##### Answer:\n",
    "$\\newcommand{\\bfzero}{\\mathbf{0}}$\n",
    "\n",
    "$$\\bff(\\bfa, \\bfb) = \\bfa + \\bfb$$ where $\\bfa, \\bfb, \\bff \\in \\bbR^n$\n",
    "\n",
    "Let $l(\\bff(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfa}(\\bfa + \\bfb)\n",
    "= \\frac{\\p l}{\\p \\bff} (\\bfI_{n \\times n} + 0_{n \\times n}) = \\frac{\\p l}{\\p \\bff}$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456390e6-260d-4979-afd2-9c342e2b6f95",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Write the vector jacobian products for vector-vector multiplication\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$$ \n",
    "\n",
    "where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in \\bbR^n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48490e5-f8ff-4372-a10f-8f5733008209",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$$ \n",
    "\n",
    "where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in \\bbR^n$\n",
    "\n",
    "Let $l(f(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product in terms of $\\frac{\\p l}{\\p f}$.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\bfa}(\\bfa^\\top \\bfb)\n",
    "= \\frac{\\p l}{\\p f} \\bfb^\\top$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\bfa^\\top$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590398ea-e3c9-41c6-994e-9c722582cd42",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "Write the vector jacobian products for for the folllowing function\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\text{sigmoid}(\\bfa^\\top \\bfb) = \n",
    "\\frac{1}{1+\\exp(-\\bfa^\\top \\bfb)}$$ \n",
    "\n",
    "where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in \\bbR^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb758760-d120-4896-927e-480126e40648",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "\n",
    "Let $l(f(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We have to find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product in terms of $\\frac{\\p l}{\\p f}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} l(f(\\bfa, \\bfb)) \n",
    "&= \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\bfa}\\left(\\frac{1}{1+\\exp(-\\bfa^\\top  \\bfb)}\\right)\\\\\n",
    "&= \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\bfa}\\left(1+\\exp(-\\bfa^\\top  \\bfb)\\right)^{-1}\\\\\n",
    "&= \\frac{\\p l}{\\p f}\\left(-1(1+\\exp(-\\bfa^\\top  \\bfb)\\right)^{-1 -1}\\frac{\\p }{\\p \\bfa} \\left(1+\\exp(-\\bfa^\\top \\bfb)\\right) \n",
    "& \\text{ Using } \\frac{\\p }{ \\p x} x^n = n x^{n-1}\\\\\n",
    "&= -\\frac{\\p l}{\\p f}\\left((1+\\exp(-\\bfa^\\top  \\bfb)\\right)^{-2}\\exp(-\\bfa^\\top \\bfb)\\frac{\\p }{\\p \\bfa} \\left(-\\bfa^\\top \\bfb\\right) \n",
    "& \\text{ Using } \\frac{\\p }{ \\p x} \\exp(x) = \\exp(x)\\\\\n",
    "&= \\frac{\\p l}{\\p f}\\left((1+\\exp(-\\bfa^\\top  \\bfb)\\right)^{-2}\\exp(-\\bfa^\\top \\bfb)\\bfb^\\top \n",
    "& \\text{ Using } \\frac{\\p }{ \\p \\bfx} \\bfx^\\top\\bfy = \\bfy^\\top\\\\\n",
    "&= \\frac{\\p l}{\\p f}\\frac{\\exp(-\\bfa^\\top \\bfb)\\bfb^\\top}{(1+\\exp(-\\bfa^\\top  \\bfb))^{2}}\n",
    "\\end{align}\n",
    "\n",
    "The other vector Jacobian product is similarly given by,\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) \n",
    "&= \\frac{\\p l}{\\p f}\\frac{\\exp(-\\bfa^\\top \\bfb)\\bfa^\\top}{(1+\\exp(-\\bfa^\\top  \\bfb))^{2}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db443e-6504-4be4-8310-7c0455c61e30",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "Write the vector jacobian products for for the folllowing function\n",
    "\n",
    "$$\\bff(\\bfa) = \\text{softmax}(\\bfa) = \\frac{\\exp(\\bfa)}{\\mathbf{1}^\\top \\exp(\\bfa)}$$ \n",
    "\n",
    "where $\\bff \\in \\bbR^n$,  and $ \\bfa \\in \\bbR^n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a82deb-1706-4f04-bc5f-ddd1960a98b1",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "\n",
    "Let $l(\\bff(\\bfa)) \\in \\bbR^n$ be the eventual scalar output. We have to find $\\frac{\\p l}{\\p \\bfa}$ for Vector Jacobian product in terms of $\\frac{\\p l}{\\p \\bff}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} l(\\bff(\\bfa)) \n",
    "&= \\frac{\\p l}{\\p \\bff} \\frac{\\p }{\\p \\bfa} \\frac{\\exp(\\bfa)}{\\bfone^\\top\\exp(\\bfa)} \\\\\n",
    "&= \\frac{\\p l}{\\p \\bff} \\frac{1}{(\\bfone^\\top\\exp(\\bfa))^2} \\left(\\bfone^\\top\\exp(\\bfa) \\frac{\\p }{\\p \\bfa} \\exp(\\bfa) - \\exp(\\bfa) \\frac{\\p }{\\p \\bfa} \\bfone^\\top\\exp(\\bfa)\\right)\n",
    "& \\text{ Using } \\left(\\frac{u}{v}\\right)' = \\frac{1}{v^2}(u' v - v'u)\n",
    "\\end{align}\n",
    "\n",
    "Let us focus on the the term $\\frac{\\p }{\\p \\bfa}\\exp(\\bfa)$,\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} \\exp(\\bfa) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p }{\\p a_1} \\exp(a_1) & \\frac{\\p }{\\p a_2} \\exp(a_1) & \\dots & \\frac{\\p }{\\p a_n} \\exp(a_1)\\\\\n",
    "\\frac{\\p }{\\p a_1} \\exp(a_2) & \\frac{\\p }{\\p a_2} \\exp(a_2) & \\dots & \\frac{\\p }{\\p a_n} \\exp(a_2)\\\\\n",
    "\\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "\\frac{\\p }{\\p a_1} \\exp(a_b) & \\frac{\\p }{\\p a_2} \\exp(a_n) & \\dots & \\frac{\\p }{\\p a_n} \\exp(a_n)\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Whenever the derivative variable and the log matches, we have $\\frac{\\p }{\\p a_i} \\exp(a_i) = \\frac{1}{a_i}$. When they are different, we have the derivative as zero. As a result we get a diagonal matrix,\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} \\exp(\\bfa) = \n",
    "\\begin{bmatrix}\n",
    "\\exp(a_1) & 0  & \\dots & 0\\\\\n",
    "0 & \\exp(a_n) & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "0 & 0 & \\dots & \\exp(a_n) \\\\\n",
    "\\end{bmatrix} = \\text{Diag}(\\exp(\\bfa))\n",
    "\\end{align}\n",
    "\n",
    "Let us call this matrix as $\\text{Diag}(\\exp(\\bfa))$. The resultant vector Jacobian product is\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} l(\\bff(\\bfa)) &= \n",
    "\\frac{\\p l}{\\p \\bff} \\frac{1}{(\\bfone^\\top\\exp(\\bfa))^2} \n",
    "\\left(\n",
    "\\bfone^\\top\\exp(\\bfa)\\text{Diag}(\\exp(\\bfa))\n",
    "- \\exp(\\bfa) \\bfone^\\top \\frac{\\p }{\\p \\bfa} \\exp(\\bfa)\n",
    "\\right)\\\\\n",
    "&=\n",
    "\\frac{\\p l}{\\p \\bff} \\frac{1}{(\\bfone^\\top\\exp(\\bfa))^2} \n",
    "\\left(\n",
    "\\bfone^\\top\\exp(\\bfa)\\text{Diag}(\\exp(\\bfa))\n",
    "- \\exp(\\bfa) \\bfone^\\top \\text{Diag}(\\exp(\\bfa))\n",
    "\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\p l}{\\p \\bff}\n",
    "\\left(\n",
    "\\bfone^\\top\\exp(\\bfa) \\bfI_n\n",
    "- \\exp(\\bfa) \\bfone^\\top\n",
    "\\right) \\frac{\\text{Diag}(\\exp(\\bfa))}{(\\bfone^\\top\\exp(\\bfa))^2},\n",
    "\\end{align}\n",
    "where $\\bfI_n$ is $n \\times n$ identity matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832bafa-1158-4d31-b028-2d141f72d943",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "Write the vector jacobian products for for the folllowing function\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\text{cross-entropy}(\\bfa, \\bfb) = \\bfa^\\top \\log(\\bfb)$$ \n",
    "\n",
    "where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in (0, 1]^n$ which means that $\\bfa$ and $\\bfb$ are n-dimensional vectors with each element between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f9030-ded1-4ae0-b5fc-57f05103d4bc",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "\n",
    "Let $l(f(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We have to find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product in terms of $\\frac{\\p l}{\\p f}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfa} l(f(\\bfa, \\bfb)) &= \\frac{\\p l}{\\p f} \\frac{\\p }{\\p \\bfa} \\bfa^\\top \\log(\\bfb) & \\text{ by chain rule} \\\\\n",
    "&=  \\frac{\\p l}{\\p f}  \\log(\\bfb)^\\top & \\text{Using } \\frac{\\p }{ \\p \\bfx} \\bfx^\\top\\bfy = \\bfy^\\top\\\\\n",
    "\\end{align}\n",
    "\n",
    "Next we compute $\\frac{\\p l}{\\p \\bfb}$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) &= \\frac{\\p l}{\\p f} \\frac{\\p }{\\p \\bfb} \\bfa^\\top \\log(\\bfb) & \\text{ by chain rule} \\\\\n",
    "&=  \\frac{\\p l}{\\p f}  \\bfa^\\top \\frac{\\p }{\\p \\bfb} \\log(\\bfb) & \\text{Using } \\frac{\\p }{ \\p \\bfx} \\bfx^\\top\\bfy = \\bfy^\\top\\\\\n",
    "&= \\frac{\\p l}{\\p f}  \\bfa^\\top \\frac{\\p }{\\p \\bfb} \\log(\\bfb)\n",
    "\\end{align}\n",
    "\n",
    "Let us focus on the last term,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfb} \\log(\\bfb) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p }{\\p b_1} \\log(b_1) & \\frac{\\p }{\\p b_2} \\log(b_1) & \\dots & \\frac{\\p }{\\p b_n} \\log(b_1)\\\\\n",
    "\\frac{\\p }{\\p b_1} \\log(b_2) & \\frac{\\p }{\\p b_2} \\log(b_2) & \\dots & \\frac{\\p }{\\p b_n} \\log(b_2)\\\\\n",
    "\\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "\\frac{\\p }{\\p b_1} \\log(b_b) & \\frac{\\p }{\\p b_2} \\log(b_n) & \\dots & \\frac{\\p }{\\p b_n} \\log(b_n)\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Whenever the derivative variable and the log matches, we have $\\frac{\\p }{\\p b_i} \\log(b_i) = \\frac{1}{b_i}$. When they are different, we have the derivative as zero. As a result we get a diagonal matrix,\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfb} \\log(\\bfb) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{b_1} & 0  & \\dots & 0\\\\\n",
    "0 & \\frac{1}{ b_2} & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "0 & 0 & \\dots & \\frac{1}{b_n} \\\\\n",
    "\\end{bmatrix} = \\text{Diag}^{-1}(\\bfb)\n",
    "\\end{align}\n",
    "\n",
    "Let us call this matrix as $\\text{Diag}^{-1}(\\bfb)$. The resultant vector Jacobian product is\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) &= \\frac{\\p l}{\\p f}  \\bfa^\\top \\text{Diag}^{-1}(\\bfb)\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
