{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Transformers and Self-Attention\n",
    "exports:\n",
    "  - format: pdf\n",
    "    template: plain_latex\n",
    "    output: exports/Transformers.pdf\n",
    "    logo: false\n",
    "    link: true\n",
    "downloads:\n",
    "  - file: exports/Transformers.pdf\n",
    "  - file: Transformers.ipynb\n",
    "math:\n",
    "    '\\calA': '{\\cal A}'\n",
    "    '\\calB': '{\\cal B}'\n",
    "    '\\calC': '{\\cal C}'\n",
    "    '\\calD': '{\\cal D}'\n",
    "    '\\calE': '{\\cal E}'\n",
    "    '\\calF': '{\\cal F}'\n",
    "    '\\calG': '{\\cal G}'\n",
    "    '\\calH': '{\\cal H}'\n",
    "    '\\calI': '{\\cal I}'\n",
    "    '\\calJ': '{\\cal J}'\n",
    "    '\\calK': '{\\cal K}'\n",
    "    '\\calL': '{\\cal L}'\n",
    "    '\\calM': '{\\cal M}'\n",
    "    '\\calN': '{\\cal N}'\n",
    "    '\\calO': '{\\cal O}'\n",
    "    '\\calP': '{\\cal P}'\n",
    "    '\\calQ': '{\\cal Q}'\n",
    "    '\\calR': '{\\cal R}'\n",
    "    '\\calS': '{\\cal S}'\n",
    "    '\\calT': '{\\cal T}'\n",
    "    '\\calU': '{\\cal U}'\n",
    "    '\\calV': '{\\cal V}'\n",
    "    '\\calW': '{\\cal W}'\n",
    "    '\\calX': '{\\cal X}'\n",
    "    '\\calY': '{\\cal Y}'\n",
    "    '\\calZ': '{\\cal Z}'\n",
    "    '\\bfa': '\\mathbf{a}'\n",
    "    '\\bfb': '\\mathbf{b}'\n",
    "    '\\bfc': '\\mathbf{c}'\n",
    "    '\\bfd': '\\mathbf{d}'\n",
    "    '\\bfe': '\\mathbf{e}'\n",
    "    '\\bff': '\\mathbf{f}'\n",
    "    '\\bfg': '\\mathbf{g}'\n",
    "    '\\bfh': '\\mathbf{h}'\n",
    "    '\\bfi': '\\mathbf{i}'\n",
    "    '\\bfj': '\\mathbf{j}'\n",
    "    '\\bfk': '\\mathbf{k}'\n",
    "    '\\bfl': '\\mathbf{l}'\n",
    "    '\\bfm': '\\mathbf{m}'\n",
    "    '\\bfn': '\\mathbf{n}'\n",
    "    '\\bfo': '\\mathbf{o}'\n",
    "    '\\bfp': '\\mathbf{p}'\n",
    "    '\\bfq': '\\mathbf{q}'\n",
    "    '\\bfr': '\\mathbf{r}'\n",
    "    '\\bfs': '\\mathbf{s}'\n",
    "    '\\bft': '\\mathbf{t}'\n",
    "    '\\bfu': '\\mathbf{u}'\n",
    "    '\\bfv': '\\mathbf{v}'\n",
    "    '\\bfw': '\\mathbf{w}'\n",
    "    '\\bfx': '\\mathbf{x}'\n",
    "    '\\bfy': '\\mathbf{y}'\n",
    "    '\\bfz': '\\mathbf{z}'\n",
    "    '\\bfW': '\\mathbf{W}'\n",
    "    '\\bfX': '\\mathbf{X}'\n",
    "    '\\bfY': '\\mathbf{Y}'\n",
    "    '\\bfZ': '\\mathbf{Z}'\n",
    "    '\\bfK': '\\mathbf{K}'\n",
    "    '\\bfQ': '\\mathbf{Q}'\n",
    "    '\\bfV': '\\mathbf{V}'\n",
    "    '\\bftheta': '\\boldsymbol{\\theta}'\n",
    "    '\\bbR': '\\mathbb{R}'\n",
    "    '\\bbE': '\\mathbb{E}'\n",
    "    '\\p': '\\partial'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Self-Attention\n",
    "\n",
    "In this notebook, we will explore:\n",
    "1. Attention as soft dictionary lookup\n",
    "2. Scaled dot-product attention\n",
    "3. Self-attention mechanism\n",
    "4. Multi-head attention\n",
    "5. Positional encoding\n",
    "6. Building a complete Transformer (MiniGPT)\n",
    "7. Training on Shakespeare text\n",
    "\n",
    "**Prerequisites**: MLP notebook (05-mlp), basic linear algebra\n",
    "\n",
    "**Reference**: \"Attention Is All You Need\" (Vaswani et al., 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install otter-grader torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup otter-grader\n",
    "URL = \"https://raw.githubusercontent.com/wecacuee/ECE490-F25-Neural-Networks/refs/heads/master/notebooks/12-transformers/TransformersTests.zip\"\n",
    "fname = \"TransformersTests.zip\"\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    urllib.request.urlretrieve(URL, fname)\n",
    "    ZipFile(fname).extractall()\n",
    "except:\n",
    "    print(\"Could not download tests. Grading may not work.\")\n",
    "import otter\n",
    "grader = otter.Notebook(tests_dir=\"./tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation: Why Attention?\n",
    "\n",
    "**Problems with RNNs:**\n",
    "- Sequential processing: $O(n)$ steps to process sequence of length $n$\n",
    "- Long-range dependencies are hard to learn\n",
    "- Vanishing/exploding gradients over long sequences\n",
    "\n",
    "**Attention's solution:**\n",
    "- Process all tokens in parallel: $O(1)$ sequential steps\n",
    "- Direct connections between any two positions\n",
    "- Gradient flows directly between any two positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention as Soft Dictionary Lookup\n",
    "\n",
    "### Hard Lookup (Traditional Dictionary)\n",
    "Given a query $\\bfq$, find the matching key and return its value:\n",
    "$$\\text{lookup}(\\bfq) = \\bfv_i \\text{ where } \\bfk_i = \\bfq$$\n",
    "\n",
    "### Soft Lookup (Attention)\n",
    "Return a **weighted average** of all values, where weights depend on query-key similarity:\n",
    "$$\\text{Attention}(\\bfq, \\bfK, \\bfV) = \\sum_{i=1}^{n} \\alpha_i \\bfv_i$$\n",
    "\n",
    "The attention weights $\\alpha_i$ are computed as:\n",
    "$$\\alpha_i = \\frac{\\exp(\\bfq^\\top \\bfk_i)}{\\sum_j \\exp(\\bfq^\\top \\bfk_j)} = \\text{softmax}(\\bfq^\\top \\bfK)_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    Simple attention mechanism for a single query.\n",
    "    \n",
    "    Args:\n",
    "        query: (d,) single query vector\n",
    "        keys: (n, d) n key vectors\n",
    "        values: (n, d_v) n value vectors\n",
    "        \n",
    "    Returns:\n",
    "        output: (d_v,) weighted sum of values\n",
    "        weights: (n,) attention weights\n",
    "    \"\"\"\n",
    "    # Compute attention scores\n",
    "    scores = query @ keys.T  # (n,)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    weights = F.softmax(scores, dim=-1)  # (n,)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = weights @ values  # (d_v,)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Example: simple key-value store\n",
    "keys = torch.tensor([[1., 0., 0.],\n",
    "                     [0., 1., 0.],\n",
    "                     [0., 0., 1.]])\n",
    "values = torch.tensor([[1., 0.],  # \"apple\"\n",
    "                       [0., 1.],  # \"banana\" \n",
    "                       [1., 1.]]) # \"cherry\"\n",
    "\n",
    "# Query close to first key\n",
    "query = torch.tensor([0.9, 0.1, 0.0])\n",
    "output, weights = simple_attention(query, keys, values)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Attention weights: {weights}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "def visualize_attention_weights(weights, query_labels, key_labels, title=\"Attention Weights\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(weights, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(range(len(key_labels)))\n",
    "    ax.set_xticklabels(key_labels)\n",
    "    ax.set_yticks(range(len(query_labels)))\n",
    "    ax.set_yticklabels(query_labels)\n",
    "    \n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.colorbar(im)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Multiple queries\n",
    "queries = torch.tensor([[0.9, 0.1, 0.0],\n",
    "                        [0.1, 0.9, 0.0],\n",
    "                        [0.3, 0.3, 0.4]])\n",
    "\n",
    "all_weights = []\n",
    "for q in queries:\n",
    "    _, w = simple_attention(q, keys, values)\n",
    "    all_weights.append(w.numpy())\n",
    "all_weights = np.array(all_weights)\n",
    "\n",
    "visualize_attention_weights(all_weights, \n",
    "                           ['query_1', 'query_2', 'query_3'],\n",
    "                           ['key_1', 'key_2', 'key_3'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "The standard attention formula used in Transformers:\n",
    "\n",
    "$$\\text{Attention}(\\bfQ, \\bfK, \\bfV) = \\text{softmax}\\left(\\frac{\\bfQ \\bfK^\\top}{\\sqrt{d_k}}\\right) \\bfV$$\n",
    "\n",
    "### Why scale by $\\sqrt{d_k}$?\n",
    "\n",
    "If $q_i, k_i \\sim \\calN(0, 1)$ independently:\n",
    "$$\\bfq^\\top \\bfk = \\sum_{i=1}^{d_k} q_i k_i$$\n",
    "\n",
    "$$\\bbE[\\bfq^\\top \\bfk] = 0, \\quad \\text{Var}(\\bfq^\\top \\bfk) = d_k$$\n",
    "\n",
    "Large variance pushes softmax into saturation (very peaked distribution). Scaling by $\\sqrt{d_k}$ normalizes variance to 1.\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Exercise 1: Implement Scaled Dot-Product Attention (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: (batch, seq_len_q, d_k) query vectors\n",
    "        K: (batch, seq_len_k, d_k) key vectors\n",
    "        V: (batch, seq_len_k, d_v) value vectors\n",
    "        mask: optional (seq_len_q, seq_len_k) mask, -inf for masked positions\n",
    "        \n",
    "    Returns:\n",
    "        output: (batch, seq_len_q, d_v) attention output\n",
    "        attention_weights: (batch, seq_len_q, seq_len_k) attention weights\n",
    "    \"\"\"\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # TODO: Implement scaled dot-product attention\n",
    "    # 1. Compute attention scores: Q @ K^T\n",
    "    # 2. Scale by sqrt(d_k)\n",
    "    # 3. If mask is provided, add it to scores (masked positions should become -inf)\n",
    "    # 4. Apply softmax to get attention weights\n",
    "    # 5. Multiply weights by V to get output\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    scores = ...  # Step 1-2\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = ...  # Step 3\n",
    "    \n",
    "    attention_weights = ...  # Step 4\n",
    "    output = ...  # Step 5\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scaled dot-product attention\n",
    "def test_scaled_attention():\n",
    "    B, T, d_k, d_v = 2, 5, 8, 16\n",
    "    Q = torch.randn(B, T, d_k)\n",
    "    K = torch.randn(B, T, d_k)\n",
    "    V = torch.randn(B, T, d_v)\n",
    "    \n",
    "    output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    assert output.shape == (B, T, d_v), f\"Expected output shape {(B, T, d_v)}, got {output.shape}\"\n",
    "    assert weights.shape == (B, T, T), f\"Expected weights shape {(B, T, T)}, got {weights.shape}\"\n",
    "    \n",
    "    # Check weights sum to 1\n",
    "    weight_sums = weights.sum(dim=-1)\n",
    "    assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5), \"Weights should sum to 1\"\n",
    "    \n",
    "    print(\"Scaled dot-product attention tests passed!\")\n",
    "\n",
    "test_scaled_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"scaled_attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention\n",
    "\n",
    "In **self-attention**, queries, keys, and values all come from the same sequence $\\bfX \\in \\bbR^{n \\times d}$:\n",
    "\n",
    "$$\\bfQ = \\bfX \\bfW^Q, \\quad \\bfK = \\bfX \\bfW^K, \\quad \\bfV = \\bfX \\bfW^V$$\n",
    "\n",
    "Where $\\bfW^Q, \\bfW^K \\in \\bbR^{d \\times d_k}$ and $\\bfW^V \\in \\bbR^{d \\times d_v}$ are learned projections.\n",
    "\n",
    "**Self-Attention Output**:\n",
    "$$\\text{SelfAttn}(\\bfX) = \\text{softmax}\\left(\\frac{\\bfX \\bfW^Q (\\bfX \\bfW^K)^\\top}{\\sqrt{d_k}}\\right) \\bfX \\bfW^V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Single-head self-attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super().__init__()\n",
    "        d_k = d_k or d_model\n",
    "        d_v = d_v or d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.d_k = d_k\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) input sequence\n",
    "            mask: optional attention mask\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_v)\n",
    "            attention_weights: (batch, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        return scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "# Test self-attention\n",
    "sa = SelfAttention(d_model=64, d_k=32, d_v=32)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out, weights = sa(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention\n",
    "\n",
    "A single attention head can only focus on one type of relationship. **Multi-head attention** runs multiple attention heads in parallel, allowing the model to attend to information from different representation subspaces.\n",
    "\n",
    "$$\\text{MultiHead}(\\bfQ, \\bfK, \\bfV) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\bfW^O$$\n",
    "\n",
    "Where each head is:\n",
    "$$\\text{head}_i = \\text{Attention}(\\bfQ \\bfW_i^Q, \\bfK \\bfW_i^K, \\bfV \\bfW_i^V)$$\n",
    "\n",
    "**Dimensions**:\n",
    "- Input: $d_{model}$\n",
    "- Per-head: $d_k = d_v = d_{model} / h$\n",
    "- Output projection: $\\bfW^O \\in \\bbR^{h \\cdot d_v \\times d_{model}}$\n",
    "\n",
    "### Exercise 2: Implement Multi-Head Attention (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention as described in \"Attention Is All You Need\".\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        n_heads: Number of attention heads\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # TODO: Define the following layers:\n",
    "        # self.W_qkv: Linear layer that projects to Q, K, V concatenated (d_model -> 3*d_model)\n",
    "        # self.W_o: Output projection (d_model -> d_model)\n",
    "        # self.dropout: Dropout layer\n",
    "        \n",
    "        self.W_qkv = ...  # YOUR CODE HERE\n",
    "        self.W_o = ...    # YOUR CODE HERE\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) input\n",
    "            mask: optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # TODO: Implement multi-head attention\n",
    "        # 1. Project x to Q, K, V using W_qkv\n",
    "        # 2. Split into n_heads: reshape to (B, T, n_heads, d_k) then transpose to (B, n_heads, T, d_k)\n",
    "        # 3. Apply scaled dot-product attention\n",
    "        # 4. Concatenate heads: transpose back and reshape to (B, T, d_model)\n",
    "        # 5. Apply output projection\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "def test_multihead():\n",
    "    B, T, d_model, n_heads = 2, 10, 64, 8\n",
    "    mha = MultiHeadAttention(d_model, n_heads)\n",
    "    x = torch.randn(B, T, d_model)\n",
    "    out = mha(x)\n",
    "    \n",
    "    assert out.shape == (B, T, d_model), f\"Expected {(B, T, d_model)}, got {out.shape}\"\n",
    "    print(\"Multi-head attention tests passed!\")\n",
    "\n",
    "test_multihead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"multihead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 6. Positional Encoding\n",
    "\n",
    "Self-attention is **permutation equivariant**: shuffling input tokens shuffles output tokens the same way. But word order matters!\n",
    "\n",
    "\"The cat sat on the mat\" vs \"The mat sat on the cat\"\n",
    "\n",
    "We inject position information using **positional encodings** added to the input embeddings.\n",
    "\n",
    "### Sinusoidal Positional Encoding\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "**Why sinusoidal?**\n",
    "- Unique encoding for each position\n",
    "- Can extrapolate to longer sequences than seen during training\n",
    "- Relative positions can be computed via linear transformation\n",
    "\n",
    "### Exercise 3: Implement Positional Encoding (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        max_len: Maximum sequence length (default: 5000)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # TODO: Create positional encoding matrix\n",
    "        # pe shape: (max_len, d_model)\n",
    "        # Use the sinusoidal formulas above\n",
    "        # Register as buffer (not a parameter) using self.register_buffer('pe', pe)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Fill in sin for even indices, cos for odd indices\n",
    "        pe[:, 0::2] = ...  # YOUR CODE HERE\n",
    "        pe[:, 1::2] = ...  # YOUR CODE HERE\n",
    "        \n",
    "        # Add batch dimension and register\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) input embeddings\n",
    "        Returns:\n",
    "            (batch, seq_len, d_model) embeddings + positional encoding\n",
    "        \"\"\"\n",
    "        # TODO: Add positional encoding to x\n",
    "        # Remember to only use positions up to seq_len\n",
    "        \n",
    "        x = ...  # YOUR CODE HERE\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "pe = PositionalEncoding(d_model=64, dropout=0.0)\n",
    "x = torch.zeros(1, 100, 64)\n",
    "pe_values = pe(x)[0].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(pe_values.T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"positional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer Block\n",
    "\n",
    "A complete Transformer block consists of:\n",
    "1. Multi-head self-attention with residual connection and layer norm\n",
    "2. Feed-forward network with residual connection and layer norm\n",
    "\n",
    "**Pre-LN variant** (used in GPT-2):\n",
    "$$\\bfx' = \\bfx + \\text{MultiHeadAttn}(\\text{LayerNorm}(\\bfx))$$\n",
    "$$\\bfx'' = \\bfx' + \\text{FFN}(\\text{LayerNorm}(\\bfx'))$$\n",
    "\n",
    "**Feed-Forward Network**:\n",
    "$$\\text{FFN}(\\bfx) = \\text{GELU}(\\bfx \\bfW_1 + \\bfb_1) \\bfW_2 + \\bfb_2$$\n",
    "\n",
    "Typically $d_{ff} = 4 \\cdot d_{model}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block with pre-layer normalization.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        n_heads: Number of attention heads\n",
    "        d_ff: Feed-forward hidden dimension (default: 4*d_model)\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-LN: LayerNorm before attention/FFN\n",
    "        x = x + self.dropout(self.attn(self.ln1(x), mask))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, n_heads=4)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out = block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Causal Masking for Language Modeling\n",
    "\n",
    "For **autoregressive** language modeling (like GPT), position $i$ can only attend to positions $\\leq i$.\n",
    "\n",
    "We achieve this with a **causal mask**:\n",
    "$$\\text{mask}_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
    "\n",
    "After softmax, future positions have zero attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) tensor with 0 for allowed positions, -inf for masked\n",
    "    \"\"\"\n",
    "    # Create upper triangular matrix of ones (excluding diagonal)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    # Replace 1s with -inf\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "mask = create_causal_mask(8)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(mask.numpy(), cmap='RdBu', vmin=-10, vmax=0)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Key position')\n",
    "plt.ylabel('Query position')\n",
    "plt.title('Causal Mask (blue=allowed, red=-inf)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building MiniGPT\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Exercise 4: Implement MiniGPT (20 points)\n",
    "\n",
    "Assemble all components into a complete GPT-style language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT-style language model.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        d_model: Model dimension (default: 128)\n",
    "        n_heads: Number of attention heads (default: 4)\n",
    "        n_layers: Number of transformer blocks (default: 4)\n",
    "        block_size: Maximum sequence length (default: 64)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=4,\n",
    "                 block_size=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # TODO: Define the following layers:\n",
    "        # self.tok_emb: Token embedding (vocab_size -> d_model)\n",
    "        # self.pos_emb: Learnable position embedding (block_size -> d_model)\n",
    "        # self.dropout: Dropout layer\n",
    "        # self.blocks: ModuleList of n_layers TransformerBlocks\n",
    "        # self.ln_f: Final LayerNorm\n",
    "        # self.head: Output projection (d_model -> vocab_size)\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying (optional but common)\n",
    "        self.tok_emb.weight = self.head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (batch, seq_len) token indices\n",
    "            targets: (batch, seq_len) target indices for loss computation\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size) output logits\n",
    "            loss: Cross-entropy loss if targets provided, else None\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size, f\"Sequence length {T} exceeds block_size {self.block_size}\"\n",
    "        \n",
    "        # TODO: Implement forward pass\n",
    "        # 1. Get token embeddings: (B, T, d_model)\n",
    "        # 2. Get position embeddings for positions 0..T-1: (T, d_model)\n",
    "        # 3. Add token + position embeddings and apply dropout\n",
    "        # 4. Create causal mask\n",
    "        # 5. Pass through all transformer blocks\n",
    "        # 6. Apply final layer norm\n",
    "        # 7. Project to vocabulary size\n",
    "        # 8. If targets provided, compute cross-entropy loss\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        tok_emb = self.tok_emb(idx)  # (B, T, d_model)\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T,)\n",
    "        pos_emb = self.pos_emb(pos)  # (T, d_model)\n",
    "        \n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = create_causal_mask(T).to(idx.device)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape for cross-entropy\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: (batch, seq_len) conditioning tokens\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "            \n",
    "        Returns:\n",
    "            (batch, seq_len + max_new_tokens) generated sequence\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size if needed\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MiniGPT\n",
    "def test_minigpt():\n",
    "    vocab_size = 100\n",
    "    model = MiniGPT(vocab_size, d_model=64, n_heads=4, n_layers=2, block_size=32)\n",
    "    \n",
    "    # Test forward pass\n",
    "    idx = torch.randint(0, vocab_size, (2, 20))\n",
    "    targets = torch.randint(0, vocab_size, (2, 20))\n",
    "    logits, loss = model(idx, targets)\n",
    "    \n",
    "    assert logits.shape == (2, 20, vocab_size), f\"Expected logits shape (2, 20, {vocab_size}), got {logits.shape}\"\n",
    "    assert loss is not None, \"Loss should not be None when targets provided\"\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = torch.randint(0, vocab_size, (1, 5))\n",
    "    generated = model.generate(prompt, max_new_tokens=10)\n",
    "    assert generated.shape == (1, 15), f\"Expected generated shape (1, 15), got {generated.shape}\"\n",
    "    \n",
    "    print(\"MiniGPT tests passed!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "test_minigpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"minigpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training on Shakespeare\n",
    "\n",
    "### Exercise 5: Train and Generate (15 points)\n",
    "\n",
    "Train MiniGPT on Shakespeare text and generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare dataset\n",
    "!mkdir -p data\n",
    "!wget -q -O data/shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Load and inspect\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level dataset for language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, text, block_size):\n",
    "        chars = sorted(set(text))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)\n",
    "        self.block_size = block_size\n",
    "        self.data = torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return torch.tensor([self.stoi[c] for c in s], dtype=torch.long)\n",
    "    \n",
    "    def decode(self, t):\n",
    "        return ''.join([self.itos[i.item()] for i in t])\n",
    "\n",
    "# Create dataset\n",
    "block_size = 64\n",
    "dataset = CharDataset(text, block_size)\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(dataset.itos.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minigpt(model, dataset, epochs=5, batch_size=64, lr=3e-4):\n",
    "    \"\"\"\n",
    "    Train MiniGPT on character dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: MiniGPT model\n",
    "        dataset: CharDataset\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        losses: List of training losses\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            logits, loss = model(x, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} complete. Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = MiniGPT(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    block_size=block_size,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train (use more epochs for better results)\n",
    "trained_model, losses = train_minigpt(model, dataset, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, dataset, prompt, max_new_tokens=200, temperature=0.8):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    prompt_enc = dataset.encode(prompt).unsqueeze(0).to(DEVICE)\n",
    "    generated = model.generate(prompt_enc, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    return dataset.decode(generated[0])\n",
    "\n",
    "# Generate with different prompts\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"The king\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    generated = generate_text(trained_model, dataset, prompt, max_new_tokens=200)\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 11. Attention Visualization\n",
    "\n",
    "Let's visualize what the attention heads are learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, text, dataset, layer=0, head=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = dataset.encode(text).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        # Output might be tuple (output, weights) or just output\n",
    "        if isinstance(output, tuple):\n",
    "            attention_weights.append(output)\n",
    "    \n",
    "    # We need to modify MultiHeadAttention to return attention weights\n",
    "    # For now, let's create a simple visualization\n",
    "    \n",
    "    # Create attention matrix for visualization\n",
    "    T = len(text)\n",
    "    Q = torch.randn(1, T, 64)\n",
    "    K = torch.randn(1, T, 64)\n",
    "    _, weights = scaled_dot_product_attention(Q, K, Q)\n",
    "    \n",
    "    # Apply causal mask\n",
    "    mask = create_causal_mask(T)\n",
    "    _, weights_causal = scaled_dot_product_attention(Q, K, Q, mask)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Without causal mask\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "    ax.set_title('Bidirectional Attention')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    \n",
    "    # With causal mask\n",
    "    ax = axes[1]\n",
    "    im = ax.imshow(weights_causal[0].detach().numpy(), cmap='Blues')\n",
    "    ax.set_title('Causal Attention (GPT-style)')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(trained_model, \"To be or not\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. BERT vs GPT Architectures\n",
    "\n",
    "| Aspect | BERT (Encoder) | GPT (Decoder) |\n",
    "|--------|----------------|---------------|\n",
    "| Attention | Bidirectional | Causal (left-to-right) |\n",
    "| Training | Masked Language Modeling | Autoregressive LM |\n",
    "| Use case | Classification, NER, QA | Text generation |\n",
    "| Mask | No mask (sees all tokens) | Causal mask |\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers):\n",
    "- Trained to predict masked tokens\n",
    "- Can see context in both directions\n",
    "- Good for understanding tasks\n",
    "\n",
    "**GPT** (Generative Pre-trained Transformer):\n",
    "- Trained to predict next token\n",
    "- Can only see past context\n",
    "- Good for generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Attention** is a soft dictionary lookup: weighted average based on query-key similarity\n",
    "2. **Scaled dot-product attention**: $\\text{softmax}(\\bfQ\\bfK^\\top/\\sqrt{d_k})\\bfV$\n",
    "3. **Self-attention**: Q, K, V all derived from the same input\n",
    "4. **Multi-head attention**: Multiple parallel attention heads capture different relationships\n",
    "5. **Positional encoding**: Inject position information (sinusoidal or learned)\n",
    "6. **Transformer block**: Multi-head attention + FFN with residual connections and layer norm\n",
    "7. **Causal masking**: Prevent attending to future tokens for autoregressive generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in order before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export\n",
    "grader.export(run_tests=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
